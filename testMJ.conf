include required(classpath("application"))

webservice {
  port = 8644
  #interface = 0.0.0.0
  #binding-timeout = 5s
  #instance.name = "reference"
}

system {
  graceful-server-shutdown = true

  # Cromwell will cap the number of running workflows at N
  #max-concurrent-workflows = 5000

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  #max-workflow-launch-count = 50

  # Number of seconds between workflow launches
  #new-workflow-poll-rate = 20

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  #number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  #number-of-cache-read-workers = 25
}

workflow-options {
  # Directory where to write per workflow logs
  workflow-log-dir: "cromwell-data/cromwell-workflow-logs"

  # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.
  # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:
  #workflow-failure-mode: "ContinueWhilePossible"
}

call-caching {
  # Allows re-use of existing results for jobs you've already run
  # (default: false)
  #enabled = false

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  #invalidate-bad-cache-results = true

  # The maximum number of times Cromwell will attempt to copy cache hits before giving up and running the job.
  #max-failed-copy-attempts = 1000000
}

docker {
  hash-lookup {
    # Set this to match your available quota against the Google Container Engine API
    #gcr-api-queries-per-100-seconds = 1000

    # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    #cache-entry-ttl = "20 minutes"

    # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    #cache-size = 200

    # How should docker hashes be looked up. Possible values are "local" and "remote"
    # "local": Lookup hashes on the local docker daemon using the cli
    # "remote": Lookup hashes on docker hub, gcr, gar, quay
    #method = "remote"
  }
}



backend {
  default: LSF
  providers: {
    LSF {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config {
        concurrent-job-limit = 10

        # `temporary-directory` creates the temporary directory for commands.
        # The expression is run from the execution directory for the script. The expression must create the directory
        temporary-directory = "$(mkdir -p cromwell-data/commands && echo cromwell-data/commands)"

	# Root directory where Cromwell writes job results.  This directory must be
        # visible and writeable by the Cromwell process as well as the jobs that Cromwell
        # launches.
	root = "cromwell-data/cromwell-executions"

        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above.
        dockerRoot = "cromwell-data/cromwell-executions"

	runtime-attributes = """
        Int cpus = 2
        Float memory_gb = 8
        String? docker
        String? job_group = "/crom/compute-cruchagac"
        """

        submit-docker = """
	  LSF_DOCKER_PORTS="" \
	  LSF_DOCKER_PRESERVE_ENVIRONMENT=false \
	  LSF_DOCKER_VOLUMES="/storage1/fs1/cruchagac/Active:/storage1/fs1/cruchagac/Active \
	  /scratch1/fs1/cruchagac:/scratch1/fs1/cruchagac \
	  ${cwd}:${docker_cwd}" \
          bsub \
            -g ${job_group} \
            -q general \
            -G compute-cruchagac \
            -R "select[mem>${memory_gb}G && ncpus>${cpus}] rusage[mem=${memory_gb}G] span[hosts=1]" \
            -cwd ${cwd} \
            -o ${out} \
            -e cromwell-data/cromwell-workflow-logs/cromwell-%J.err \
	    -N \
            -M ${memory_gb}G \
            -a "docker(${docker})" ${docker_script}
        """
        kill = "bkill ${job_id}"
	docker-kill = "bkill ${job_id}"
        check-alive = "bjobs -noheader -o stat ${job_id} | /bin/grep 'PEND\\|RUN'"
        job-id-regex = "Job <(\\d+)>.*"
      }
    }
  }
}
